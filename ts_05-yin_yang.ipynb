{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f7ba80",
   "metadata": {},
   "source": [
    "# Training an SNN on BrainScaleS-2 with PyTorch\n",
    "\n",
    "In this tutorial we create and train a spiking neural network (SNN) on\n",
    "the neuromorphic hardware system BrainScaleS-2 (BSS-2) [1] to solve the\n",
    "Yin-Yang dataset [2] using the PyTorch-based software framwork\n",
    "`hxtorch.snn` [3]. For training we rely on surrogate gradients [4].\n",
    "\n",
    "This tutorial assumes you are familiar with the basics of\n",
    "`hxtorch.snn` and the training of SNNs with surrogate gradients.\n",
    "\n",
    "For further reading, see the references below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392aca9",
   "metadata": {},
   "source": [
    "## References and further reading\n",
    "\n",
    "[1] Pehle, C., Billaudelle, S., Cramer, B., Kaiser, J., Schreiber, K.,\n",
    "Stradmann, Y., Weis, J., Leibfried, A., Müller, E., and Schemmel, J. The\n",
    "BrainScaleS-2 accelerated neuromorphic system with hybrid plasticity.\n",
    "Frontiers in Neuroscience, 16, 2022. ISSN 1662-453X. [doi:\n",
    "10.3389/fnins.2022.795876](https://www.frontiersin.org/articles/10.3389/fnins.2022.795876/full).\n",
    "\n",
    "[2] Kriener, L., Göltz, J., and Petrovici, M. A. The yin-yang dataset.\n",
    "In Neuro-Inspired Computational Elements Conference, NICE 2022,\n",
    "pp. 107–111, New York, NY, USA, 2022. Association for Computing\n",
    "Machinery. ISBN 9781450395595. [doi:\n",
    "10.1145/3517343.3517380](https://dl.acm.org/doi/10.1145/3517343.3517380).\n",
    "\n",
    "[3] Spilger, P., Arnold, E., Blessing, L., Mauch, C., Pehle, C., Müller,\n",
    "E., and Schemmel, J. hxtorch.snn: Machine- learning-inspired spiking\n",
    "neural network modeling on BrainScaleS-2. Accepted, 2023. [doi:\n",
    "10.48550.2212.12210](https://doi.org/10.48550/arXiv.2212.12210)\n",
    "\n",
    "[4] Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. 2019.\n",
    "Surrogate gradi- ent learning in spiking neural networks: Bringing the\n",
    "power of gradient-based optimization to spiking neural networks. IEEE\n",
    "Signal Processing Magazine 36, 6 (2019),51–63.\n",
    "[https://doi.org/10.1109/MSP.2019.2931595](https://doi.org/10.1109/MSP.2019.2931595)\n",
    "\n",
    "[5] Wunderlich, T.C., Pehle, C. Event-based backpropagation can compute exact gradients\n",
    "for spiking neural networks. Scientific Reports 11, 12829 (2021).\n",
    "[doi: 10.1038/s41598-021-91786-z](https://doi.org/10.1038/s41598-021-91786-z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c909d6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from typing import Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as w\n",
    "import numpy as np\n",
    "import torch\n",
    "from _static.common.helpers import setup_hardware_client, save_nightly_calibration\n",
    "from _static.tutorial.snn_yinyang_helpers import plot_data, plot_input_encoding, plot_training\n",
    "setup_hardware_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f5546",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Some seeds\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c9dc79",
   "metadata": {},
   "source": [
    "### Yin-Yang Dataset\n",
    "\n",
    "The Yin-Yang dataset is a small two-dimensional nonlinear dataset to be\n",
    "solved with limited resources where the sharp boundaries between the\n",
    "classes - despite the low input dimensionality - create a hard problem\n",
    "that neatly separates SNNs according to their capabilities. It consists\n",
    "of three classes: the yin, the yang and dots. Each data sample $ i $ is\n",
    "defined by its coordinates $ (x_i, y_i) $, assigned to one of the three\n",
    "classes, depending on the area it is located in. Further information can\n",
    "be found in [2].\n",
    "\n",
    "First, we import the YinYangDataset and create a PyTorch DataLoader to\n",
    "access and visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bc578",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from hxtorch.snn.datasets.yinyang import YinYangDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "R_BIG = 0.5  # Radius of yin-yang sign\n",
    "R_SMALL = 0.1  # Radius of yin-yang eyes\n",
    "\n",
    "def get_data_loaders(\n",
    "        trainset_size: int, testset_size: int = 1, batch_size: int = 75):\n",
    "    \"\"\"\n",
    "    Create a training and a test Yin-Yang dataset and return corresponding PyTorch\n",
    "    data loaders.\n",
    "    :param trainset_size: Number of samples in the training set.\n",
    "    :param testset_size: Number of samples in the testset.\n",
    "    :param batch_size: The batch size, used for both, the train and the test loader.\n",
    "    :return: Returns a train and test data loader.\n",
    "    \"\"\"\n",
    "    trainset = YinYangDataset(r_big=R_BIG, r_small=R_SMALL, size=trainset_size)\n",
    "    testset = YinYangDataset(r_big=R_BIG, r_small=R_SMALL, size=testset_size)\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078ecbf",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "train_loader, _ = get_data_loaders(1000, 10, batch_size=1000)\n",
    "train_loader, len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06989a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Get data and targets\n",
    "data, targets = next(iter(train_loader))\n",
    "data[:10], targets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8578ef",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# One random example for which we want to look at its spike encoding\n",
    "example = data[np.random.randint(0, len(data))]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22022e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_data(example, data, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30bc38",
   "metadata": {},
   "source": [
    "### SNN Model\n",
    "\n",
    "We now define an SNNs which we want to train to classify the class of a\n",
    "given sample. For that we use an SNN with one hidden leaky-integrate and\n",
    "fire (LIF) layer projecting its spike events onto one leaky-integrator\n",
    "(LI) readout layer, as in [3]. Each neuron in the output layer\n",
    "corresponds to one of the three classes: ying, yang and dot.\n",
    "\n",
    "To avoid time-consuming implicit calibration from given “Neuron” parameters we\n",
    "load a prepared calibration. Note, that changing “Neuron” hardware parameters\n",
    "becomes not effective. You can comment out the lines loading the calibration\n",
    "if you want to use different parameters but be aware than the this will trigger\n",
    "a hardware calibration which might take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccac87e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import hxtorch\n",
    "import hxtorch.snn as hxsnn\n",
    "import hxtorch.snn.functional as F\n",
    "from hxtorch.snn.transforms import weight_transforms\n",
    "from dlens_vx_v3 import halco\n",
    "\n",
    "log = hxtorch.logger.get(\"grenade.backend\")\n",
    "hxtorch.logger.default_config(level=hxtorch.logger.LogLevel.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca99fa8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class SNN(torch.nn.Module):\n",
    "    \"\"\" SNN with one hidden LIF layer and one readout LI layer \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_in: int,\n",
    "            n_hidden: int,\n",
    "            n_out: int,\n",
    "            mock: bool,\n",
    "            dt: float,\n",
    "            tau_mem: float,\n",
    "            tau_syn: float,\n",
    "            alpha: float,\n",
    "            trace_shift_hidden: int,\n",
    "            trace_shift_out: int,\n",
    "            weight_init_hidden: Tuple[float, float],\n",
    "            weight_init_output: Tuple[float, float],\n",
    "            weight_scale: float,\n",
    "            trace_scale: float,\n",
    "            input_repetitions: int,\n",
    "            device: torch.device):\n",
    "        \"\"\"\n",
    "        :param n_in: Number of input units.\n",
    "        :param n_hidden: Number of hidden units.\n",
    "        :param n_out: Number of output units.\n",
    "        :param mock: Indicating whether to train in software or on hardware.\n",
    "        :param dt: Time-binning width.\n",
    "        :param tau_mem: Membrane time constant.\n",
    "        :param tau_syn: Synaptic time constant.\n",
    "        :param trace_shift_hidden: Indicates how many indices the membrane\n",
    "            trace of hidden layer is shifted to left along time axis.\n",
    "        :param trace_shift_out: Indicates how many indices the membrane\n",
    "            trace of readout layer is shifted to left along time axis.\n",
    "        :param weight_init_hidden: Hidden layer weight initialization mean\n",
    "            and std value.\n",
    "        :param weight_init_output: Output layer weight initialization mean\n",
    "            and std value.\n",
    "        :param weight_scale: The factor with which the software weights are\n",
    "            scaled when mapped to hardware.\n",
    "        :param input_repetitions: Number of times to repeat input channels.\n",
    "        :param device: The used PyTorch device used for tensor operations in\n",
    "            software.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dt = dt\n",
    "\n",
    "        # Instance to work on\n",
    "        self.n_in = n_in\n",
    "        self.n_hidden = n_hidden\n",
    "        self.tau_mem = tau_mem\n",
    "        self.tau_syn = tau_syn\n",
    "        self.input_repetitions = input_repetitions\n",
    "        self.weight_scale = weight_scale\n",
    "        self.trace_scale = trace_scale\n",
    "        self.trace_shift_hidden = trace_shift_hidden\n",
    "\n",
    "        # Experiment instance\n",
    "        self.experiment = hxsnn.Experiment(mock=mock, dt=dt)\n",
    "        # To avoid time-consuming implicit calibration from given parameters\n",
    "        # we load a prepared calibration. Note, that changing \"Neuron\"\n",
    "        # hardware parameters become ineffective\n",
    "        save_nightly_calibration('spiking2_calix-native.pkl')\n",
    "        self.experiment.default_execution_instance.load_calib(\n",
    "            \"spiking2_calix-native.pkl\")\n",
    "\n",
    "        # Repeat input\n",
    "        self.input_repetitions = input_repetitions\n",
    "\n",
    "        # Input projection\n",
    "        self.linear_h = hxsnn.Synapse(\n",
    "            n_in * input_repetitions,\n",
    "            n_hidden,\n",
    "            experiment=self.experiment,\n",
    "            transform=partial(\n",
    "                weight_transforms.linear_saturating, scale=weight_scale))\n",
    "\n",
    "        # Initialize weights\n",
    "        if weight_init_hidden:\n",
    "            w = torch.zeros(n_hidden, n_in)\n",
    "            torch.nn.init.normal_(w, *weight_init_hidden)\n",
    "            self.linear_h.weight.data = w.repeat(1, input_repetitions)\n",
    "\n",
    "        # Hidden layer\n",
    "        self.lif_h = hxsnn.Neuron(\n",
    "            n_hidden,\n",
    "            experiment=self.experiment,\n",
    "            tau_mem=tau_mem,\n",
    "            tau_syn=tau_syn,\n",
    "            leak=hxsnn.MixedHXModelParameter(0., 80),\n",
    "            reset=hxsnn.MixedHXModelParameter(0., 80),\n",
    "            threshold=hxsnn.MixedHXModelParameter(1., 150),\n",
    "            i_synin_gm=500,\n",
    "            synapse_dac_bias=1000,\n",
    "            trace_scale=trace_scale,\n",
    "            cadc_time_shift=trace_shift_hidden,\n",
    "            shift_cadc_to_first=True)\n",
    "\n",
    "        # Output projection\n",
    "        self.linear_o = hxsnn.Synapse(\n",
    "            n_hidden,\n",
    "            n_out,\n",
    "            experiment=self.experiment,\n",
    "            transform=partial(\n",
    "                weight_transforms.linear_saturating, scale=weight_scale))\n",
    "\n",
    "        # Readout layer\n",
    "        self.li_readout = hxsnn.ReadoutNeuron(\n",
    "            n_out,\n",
    "            experiment=self.experiment,\n",
    "            tau_mem=tau_mem,\n",
    "            tau_syn=tau_syn,\n",
    "            leak=hxsnn.MixedHXModelParameter(0., 80),\n",
    "            i_synin_gm=500,\n",
    "            synapse_dac_bias=1000,\n",
    "            trace_scale=trace_scale,\n",
    "            cadc_time_shift=trace_shift_out,\n",
    "            shift_cadc_to_first=True,\n",
    "            placement_constraint=list(\n",
    "                halco.LogicalNeuronOnDLS(\n",
    "                    hxsnn.morphology.SingleCompartmentNeuron(1).compartments,\n",
    "                    halco.AtomicNeuronOnDLS(\n",
    "                        halco.NeuronRowOnDLS(1), halco.NeuronColumnOnDLS(nrn)))\n",
    "                for nrn in range(n_out)))\n",
    "\n",
    "        # Initialize weights\n",
    "        if weight_init_output:\n",
    "            torch.nn.init.normal_(self.linear_o.weight, *weight_init_output)\n",
    "\n",
    "        # Device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, spikes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward path.\n",
    "        :param spikes: NeuronHandle holding spikes as input.\n",
    "        :return: Returns the output of the network, i.e. membrane traces of the\n",
    "        readout neurons.\n",
    "        \"\"\"\n",
    "        # Remember input spikes for plotting\n",
    "        self.s_in = spikes\n",
    "        # Increase synapse strength by repeating each input\n",
    "        spikes = spikes.repeat(1, 1, self.input_repetitions)\n",
    "        # Spike input handle\n",
    "        spikes_handle = hxsnn.NeuronHandle(spikes)\n",
    "\n",
    "        # Forward\n",
    "        c_h = self.linear_h(spikes_handle)\n",
    "        self.s_h = self.lif_h(c_h)  # Keep spikes for fire reg.\n",
    "        c_o = self.linear_o(self.s_h)\n",
    "        self.y_o = self.li_readout(c_o)\n",
    "\n",
    "        # Execute on hardware\n",
    "        hxtorch.snn.run(self.experiment, spikes.shape[0])\n",
    "\n",
    "        return self.y_o.membrane_cadc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887868e7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N_HIDDEN      = 120\n",
    "MOCK          = False\n",
    "DT            = 2.0e-06  # s\n",
    "\n",
    "# We need to specify the device we want to use on the host computer\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# The SNN\n",
    "snn = SNN(\n",
    "    n_in=5,\n",
    "    n_hidden=N_HIDDEN,\n",
    "    n_out=3,\n",
    "    mock=MOCK,\n",
    "    dt=DT,\n",
    "    tau_mem=6.0e-06,\n",
    "    tau_syn=6.0e-06,\n",
    "    alpha=50.,\n",
    "    trace_shift_hidden=int(.0e-06/DT),\n",
    "    trace_shift_out=int(.0e-06/DT),\n",
    "    weight_init_hidden=(0.001, 0.25),\n",
    "    weight_init_output=(0.0, 0.1),\n",
    "    weight_scale=66.39,\n",
    "    trace_scale=0.0147,\n",
    "    input_repetitions=1 if MOCK else 5,\n",
    "    device=device)\n",
    "snn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984885a1",
   "metadata": {},
   "source": [
    "Since the SNN gets spike events as inputs and the samples from the\n",
    "dataset are real-valued, we first need to translate them into a\n",
    "spike-based representation by an `encoder` module before we can pass\n",
    "them to the SNN. Additionally, the we need to define some decoder\n",
    "functionallity that translates the output of the SNN, here the trace of\n",
    "the LI layer, into class scores to infere a prediction from. This is\n",
    "done by an `decoder` module. For easier handling, the `encoder`, the\n",
    "`snn`, and the `decoder` are wrapped into a `Model` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b38bd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \"\"\" Complete model with encoder, network (snn) and decoder \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder: torch.nn.Module,\n",
    "            network: torch.nn.Module,\n",
    "            decoder: torch.nn.Module,\n",
    "            readout_scale: float = 1.):\n",
    "        \"\"\"\n",
    "        Initialize the model by assigning encoder, network and decoder\n",
    "        :param encoder: Module to encode input data\n",
    "        :param network: Network module containing layers and\n",
    "            parameters / weights\n",
    "        :param decoder: Module to decode network output\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.network = network\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.readout_scale = readout_scale\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform forward pass through whole model, i.e.\n",
    "        data -> encoder -> network -> decoder -> output\n",
    "        :param inputs: tensor input data\n",
    "        :returns: Returns tensor output\n",
    "        \"\"\"\n",
    "        spikes = self.encoder(inputs)\n",
    "        traces = self.network(spikes)\n",
    "        self.scores = self.decoder(traces).clone()\n",
    "\n",
    "        # scale outputs\n",
    "        with torch.no_grad():\n",
    "            self.scores *= self.readout_scale\n",
    "\n",
    "        return self.scores\n",
    "\n",
    "    def regularize(\n",
    "            self,\n",
    "            reg_readout: float = 0.0,\n",
    "            reg_bursts: float = 0.0,\n",
    "            reg_w_hidden: float = 0.0,\n",
    "            reg_w_output: float = 0.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get regularization terms for bursts and weights like\n",
    "        factor * (thing to be regularized) ** 2.\n",
    "        :param reg_bursts: prefactor of burst / hidden spike regulaization\n",
    "        :param reg_weights_hidden: prefactor of hidden weight regularization\n",
    "        :param reg_weights_output: prefactor of output weight regularization\n",
    "        :returns: Returns sum of regularization terms\n",
    "        \"\"\"\n",
    "        reg = torch.tensor(0., device=self.scores.device)\n",
    "        # Reg readout\n",
    "        reg += reg_readout * torch.mean(self.scores ** 2)\n",
    "        # bursts (hidden spikes) regularization\n",
    "        reg += reg_bursts * torch.mean(\n",
    "            torch.sum(self.network.s_h.spikes, dim=1) ** 2.)\n",
    "        # weight regularization\n",
    "        reg += reg_w_hidden * torch.mean(self.network.linear_h.weight ** 2.)\n",
    "        reg += reg_w_output * torch.mean(self.network.linear_o.weight ** 2.)\n",
    "        return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc2575",
   "metadata": {},
   "source": [
    "If we want to use an SNN to classify a sample $ i $ in the Yin-Yang\n",
    "dataset, we have to translate the point $ (x_i, y_i) $ to spikes. For\n",
    "this, we translate the value in each dimension, as well as their\n",
    "inverse, to a spike time $ t_n^i $ of an input neuron $ n $ into\n",
    "a range $ [t_\\text{early}, t_\\text{late}] $ [2]:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    x_{i} \\\\\n",
    "    y_{i} \\\\\n",
    "    1 - x_{i} \\\\\n",
    "    1 - y_{i} \\\\\n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\n",
    "\\begin{bmatrix}\n",
    "    t^i_0 \\\\\n",
    "    t^i_1 \\\\\n",
    "    t^i_2 \\\\\n",
    "    t^i_3\n",
    "\\end{bmatrix}\n",
    "= t_\\text{early} +\n",
    "\\begin{bmatrix}\n",
    "    x_{i} \\\\\n",
    "    y_{i} \\\\\n",
    "    1 - x_{i} \\\\\n",
    "    1 - y_{i}\n",
    "\\end{bmatrix}\n",
    "\\left( t_\\text{late} - t_\\text{early} \\right)\n",
    "$$\n",
    "\n",
    ".\n",
    "\n",
    "To increase activity in the network we add an additional input neuron\n",
    "that has a constant firing time $ t^\\text{bias} $, such\n",
    "that sample $ i $ is represented by the spike events $ (t^i_0,\n",
    "t^i_1, t^i_2, t^i_3, t^\\text{bias}_4)^\\top $.\n",
    "\n",
    "The dataset `YinYangDataset` returns each data point in the form\n",
    "$ (x_i, y_i, 1-x_i, 1-y_i) $. To translate them into spike times we\n",
    "use the encoder module `CoordinatesToSpikes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d2657",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from hxtorch.snn.transforms.encode import CoordinatesToSpikes\n",
    "\n",
    "T_SIM   = 6.0e-05  # s\n",
    "T_EARLY = 2.0e-06  # s\n",
    "T_LATE  = 4.0e-05  # s\n",
    "T_BIAS  = 1.8e-05  # s\n",
    "\n",
    "# This encoder translates the points into spikes on a discrete time lattice\n",
    "encoder = CoordinatesToSpikes(\n",
    "    seq_length=int(T_SIM / DT),\n",
    "    t_early=T_EARLY,\n",
    "    t_late=T_LATE,\n",
    "    dt=DT,\n",
    "    t_bias=T_BIAS)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d59465f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "spikes = encoder(example.unsqueeze(0)).squeeze(1)\n",
    "spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e2b56",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_input_encoding(spikes.cpu(), T_EARLY, T_LATE, T_BIAS, T_SIM, DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89560e",
   "metadata": {},
   "source": [
    "As `decoder` we use the max-over-time function, which returns the\n",
    "highest membrane value along the time for each output neuron in the LI\n",
    "layer. Those max-over-time-values are interpreted as scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0e609",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from hxtorch.snn.transforms.decode import MaxOverTime\n",
    "decoder = MaxOverTime()\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f400b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model = Model(encoder, snn, decoder, readout_scale=10.)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce2d26",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We now create a training routine in a PyTorch fashion. We use the Adam\n",
    "optimizer for weight optimization and the cross-entropy as loss\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca1b54",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def predict(model, data, target, loss_func):\n",
    "    \"\"\" \"\"\"\n",
    "    scores = model(data)\n",
    "    loss = model.regularize(reg_readout=0.0004)\n",
    "    loss = loss_func(scores, target) + loss\n",
    "    return scores, loss\n",
    "\n",
    "\n",
    "def stats(model, scores, target):\n",
    "    \"\"\" \"\"\"\n",
    "    # Train accuracy\n",
    "    pred = scores.cpu().argmax(dim=1)\n",
    "    acc = pred.eq(target.view_as(pred)).float().mean().item()\n",
    "    # Firing rates\n",
    "    rate = model.network.s_h.spikes.sum().item() / scores.shape[0]\n",
    "    return acc, rate\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          loader: DataLoader,\n",
    "          loss_func: torch.nn.CrossEntropyLoss,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          epoch: int, update):\n",
    "    \"\"\"\n",
    "    Perform training for one epoch.\n",
    "    :param model: The model to train.\n",
    "    :param loader: Pytorch DataLoader instance providing training data.\n",
    "    :param optimizer: The optimizer used or weight optimization.\n",
    "    :param epoch: Current epoch for logging.\n",
    "    :returns: Tuple (training loss, training accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss, acc = 0., 0.\n",
    "    n_total = len(loader)\n",
    "\n",
    "    pbar = tqdm(total=len(loader), unit=\"batch\", leave=False)\n",
    "    for data, target in loader:\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        scores, loss_b = predict(model, data.to(device), target.to(device), loss_func)\n",
    "\n",
    "        loss_b.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_b, rate_b = stats(model, scores, target)\n",
    "\n",
    "        acc += acc_b / n_total\n",
    "        loss += loss_b.item() / n_total\n",
    "\n",
    "        update(n_total, loss_b.item(), 100 * acc_b, rate_b)\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            epoch=f\"{epoch}\", loss=f\"{loss_b.item():.4f}\", acc=f\"{acc_b:.4f}\",\n",
    "            rate=f\"{rate_b:.2f}\", lr=f\"{optimizer.param_groups[-1]['lr']}\")\n",
    "        pbar.update()\n",
    "    pbar.close()\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def test(model: torch.nn.Module,\n",
    "         loader: torch.utils.data.DataLoader,\n",
    "         loss_func: torch.nn.CrossEntropyLoss,\n",
    "         epoch: int, update):\n",
    "    \"\"\"\n",
    "    Test the model.\n",
    "    :param model: The model to test\n",
    "    :param loader: Data loader containing the test data set\n",
    "    :param epoch: Current trainings epoch.\n",
    "    :returns: Tuple of (test loss, test accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dev = model.network.device\n",
    "\n",
    "    loss, acc, rate = 0., 0., 0\n",
    "    data, target, scores = [], [], []\n",
    "    n_total = len(loader)\n",
    "\n",
    "    pbar = tqdm(total=len(loader), unit=\"batch\", leave=False)\n",
    "    for data_b, target_b in loader:\n",
    "        scores_b, loss_b = predict(model, data_b.to(device), target_b.to(device), loss_func)\n",
    "        scores.append(scores_b.detach())\n",
    "        data.append(data_b.detach())\n",
    "        target.append(target_b.detach())\n",
    "\n",
    "        acc_b, rate_b = stats(model, scores_b, target_b)\n",
    "        acc += acc_b / n_total\n",
    "        loss += loss_b.item() / n_total\n",
    "        rate += rate_b / n_total\n",
    "\n",
    "        pbar.update()\n",
    "    pbar.close()\n",
    "    print(f\"Test epoch: {epoch}, average loss: {loss:.4f}, test acc={100 * acc:.2f}%\")\n",
    "\n",
    "    scores = torch.stack(scores).reshape(-1, 3).cpu()\n",
    "    data = torch.stack(data).reshape(-1, 4).cpu()\n",
    "    target = torch.stack(target).reshape(-1).cpu()\n",
    "\n",
    "    update(\n",
    "        model.network.s_in.detach().cpu(),\n",
    "        model.network.s_h.spikes.detach().cpu(),\n",
    "        model.network.y_o.membrane_cadc.detach().cpu(),\n",
    "        data, target, scores,\n",
    "        loss, 100 * acc, rate)\n",
    "\n",
    "    return loss, acc, rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda29e5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Training params\n",
    "LR            = 0.002\n",
    "STEP_SIZE     = 5\n",
    "GAMMA         = 0.9\n",
    "EPOCHS        = 4 # Adjust here for longer training...\n",
    "BATCH_SIZE    = 75\n",
    "TRAINSET_SIZE = 5025\n",
    "TESTSET_SIZE  = 1050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0cd3d6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Just for plotting...\n",
    "assert TRAINSET_SIZE % BATCH_SIZE == 0\n",
    "\n",
    "# PyTorch stuff... optimizer, scheduler and loss like you normally do.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Data loaders\n",
    "train_loader, test_loader = get_data_loaders(TRAINSET_SIZE, TESTSET_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b052f05",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Functions to update plot\n",
    "update_plot, update_train_data, update_test_data = plot_training(N_HIDDEN, T_SIM, DT)\n",
    "plt.close()\n",
    "output = w.Output()\n",
    "display(output)\n",
    "\n",
    "# Initialize the hardware\n",
    "if not MOCK:\n",
    "    hxtorch.init_hardware()\n",
    "\n",
    "# Train and test\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    # Test\n",
    "    loss_test, acc_test, rate_test = test(\n",
    "        model, test_loader, loss, epoch, update_test_data)\n",
    "\n",
    "    # Refresh plot\n",
    "    output.clear_output(wait=True)\n",
    "    with output:\n",
    "        update_plot()\n",
    "\n",
    "    # Train epoch\n",
    "    if epoch < EPOCHS:\n",
    "        loss_train, acc_train = train(\n",
    "            model, train_loader, loss, optimizer, epoch, update_train_data)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# Release the hardware connection\n",
    "hxtorch.release_hardware()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbc5ea",
   "metadata": {},
   "source": [
    "### EventProp\n",
    "\n",
    "In [5] Wunderlich and Pehle derived the EventProp algorithm, which provides\n",
    "a set of equations to compute exact parameter gradients for spiking neural\n",
    "networks with LIF neurons, single-exponential-shaped synpases and a quite\n",
    "general loss function.\n",
    "\n",
    "The background section below is meant to give an overview of the equations\n",
    "in the EventProp algorthm and provide a basis to understand the\n",
    "time-discretized implementation in PyTorch autograd functions below.\n",
    "This is all based directly on [5] and for the detailed derivation of the\n",
    "algorithm, you might look into the reference.\n",
    "\n",
    "If you just want to use the functions and train the network using them, you\n",
    "might skip directly to the training part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff29ac",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "The state of a neuron $ n $ is given by its membrane potential\n",
    "$ V_{n} $ and synaptic current $ I_{n} $, and their dynamics are\n",
    "governed by a set of coupled differential equations\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & \\text{Free dynamics}                  && \\quad \\text{Transition condition}              && \\quad \\text{Jumps at transition}   \\\\\n",
    "    &\\tau_{\\mathrm{m}} \\dot{V} = - V + I    && \\quad (V)_{n} - V_{\\mathrm{th}} = 0 \\text{, }(\\dot{V})_{n} > 0    && \\quad (V^{+})_{n} = 0              \\\\\n",
    "    &\\tau_{\\mathrm{s}} \\dot{I} = - I        && \\quad \\text{for any } n                        && \\quad I^{+} = I^{-} + W e_{n}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where the superscripts $ + $ and $ - $ denote the right- and left-hand\n",
    "limit to the post-synaptic spike time.\n",
    "\n",
    "The loss, which is to be minimized, is of the form\n",
    "\n",
    "$$\n",
    "L = l_{\\mathrm{p}} (t^{\\mathrm{post}}) + \\int_{0}^{T} l_{V} (V, t) \\mathrm{d}t,\n",
    "$$\n",
    "\n",
    "where $ l_{\\mathrm{p}}(t^{\\mathrm{post}}) $ and $ l_{V} (V, t) $ are\n",
    "smooth loss functions depending on the membrane potentials $ V $, time\n",
    "$ t $ and set of post-synaptic spike times $ t^{\\mathrm{post}} $.\n",
    "\n",
    "The system’s forward dynamics, defined in the table above, can be introduced\n",
    "as constraints via Lagrange multipliers $ \\lambda_{V} $ and $ \\lambda_{I} $,\n",
    "referring to the equation of the respective state variable. From this, an adjoint\n",
    "system of differential equations for the lagrange multipliers can be found and\n",
    "solved in reverse time. They also undergo jumps at the spike times of neurons\n",
    "found by solving (or in our case emulating) the forward dynamics. Using the\n",
    "notation $ ' = - \\frac{\\mathrm{d}}{\\mathrm{d} t} $, the adjoint equations are\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & \\text{Free dynamics} && \\quad \\text{Transition condition} && \\quad \\text{Jumps at transition} \\\\\n",
    "    & \\tau_{\\mathrm{m}} \\lambda^{\\prime}_{V} = - \\lambda_{V} + \\frac{\\partial l_{V}}{\\partial V}\n",
    "    && \\quad t - t^{\\mathrm{post}}_{k} = 0\n",
    "    && \\quad \\left(\\lambda_{V}^{-} \\right)_{n(k)} = \\left(\\lambda_{V}^{+} \\right)_{n(k)} + \\frac{1}{\\tau_{\\mathrm{m}} (\\dot{V}^{-})_{n(k)} } \\bigg[ \\vartheta \\left(\\lambda_{V}^{+} \\right)_{n(k)} \\\\\n",
    "    & \\tau_{\\mathrm{s}} \\lambda^{\\prime}_{I} = - \\lambda_{I} + \\lambda_{V}\n",
    "    && \\quad \\text{for any } k\n",
    "    && \\quad\\quad + \\left( W^{\\top} \\left( \\lambda_{V}^{+} - \\lambda_{I} \\right) \\right) + \\frac{\\partial l_{\\mathrm{post}}}{\\partial t^{\\mathrm{post}}_{k}} + l_{V}^{-} - l_{V}^{+} \\bigg]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The gradient with respect to\n",
    "the synaptic weight $ w_{ji} $, connecting pre-synaptic neuron $ i $\n",
    "to post-synaptic neuron $ j $, then only depends on the syaptic time\n",
    "constant $ \\tau_{\\mathrm{s}} $ and the adjoint variable $ \\lambda_{I} $\n",
    "at spike times:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} L}{\\mathrm{d}w_{ji}} = - \\tau_{\\mathrm{s}}\n",
    "\\sum_{\\text{spikes from } i} (\\lambda_{I})_{j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd3d226",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "The neuron and synapse modules in `hxtorch.snn` allow users to provide\n",
    "custom functions and we use this ability to implement the EventProp\n",
    "algorithm [5] as an alternative gradient estimator to the surrogate\n",
    "gradients which are used in the part above.\n",
    "\n",
    "To ensure appropriate backpropagation of the terms in the EventProp\n",
    "equations between layers one has to provide two functions handling\n",
    "the computation and propagation of gradients, one for `Neuron` layer\n",
    "and one for the `Synapse` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022fccb0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "\n",
    "class EventPropNeuronFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gradient estimation with time-discretized EventProp using explicit Euler integration.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx,\n",
    "                input: torch.Tensor,\n",
    "                leak: torch.Tensor,\n",
    "                reset: torch.Tensor,\n",
    "                threshold: torch.Tensor,\n",
    "                tau_syn: torch.Tensor,\n",
    "                tau_mem: torch.Tensor,\n",
    "                hw_data: Optional[torch.Tensor] = None,\n",
    "                dt: float = 1e-6) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward function, generating spikes at positions > 0.\n",
    "\n",
    "        :param input: Weighted input spikes in shape (2, batch, time, neurons).\n",
    "            The 2 at dim 0 comes from stacked output in EventPropSynapse.\n",
    "        :param leak: The leak voltage as torch.Tensor.\n",
    "        :param reset: The reset voltage as torch.Tensor.\n",
    "        :param threshold: The threshold voltage as torch.Tensor.\n",
    "        :param tau_syn: The synaptic time constant as torch.Tensor.\n",
    "        :param tau_mem: The membrane time constant as torch.Tensor.\n",
    "        :param dt: Step width of integration.\n",
    "        :param hw_data: Optionally available observables from hardware.\n",
    "\n",
    "        :returns: Returns the spike trains and membrane trace.\n",
    "            Both tensors are of shape (batch, time, neurons).\n",
    "        \"\"\"\n",
    "        dev = input.device\n",
    "        # If hardware observables are given, return them directly.\n",
    "        if hw_data is not None:\n",
    "            ctx.extra_kwargs = {\n",
    "                \"params\": (leak, reset, threshold, tau_syn, tau_mem), \"dt\": dt}\n",
    "            hw_data = tuple(\n",
    "                data.to(dev) if data is not None else None for data in hw_data)\n",
    "            ctx.save_for_backward(input, *hw_data)\n",
    "            return hw_data\n",
    "\n",
    "        # Otherwise integrate the neuron dynamics in software\n",
    "        T, bs, ps = input[0].shape\n",
    "        z, i = torch.zeros(bs, ps).to(dev), torch.zeros(bs, ps).to(dev)\n",
    "        v = torch.empty(bs, ps, device=dev)\n",
    "        v[:, :] = leak\n",
    "        spikes, current, membrane = [z], [i], [v]\n",
    "        for ts in range(T - 1):\n",
    "            # Current\n",
    "            i = i * (1 - dt / tau_syn) + input[0][ts]\n",
    "            current.append(i)\n",
    "\n",
    "            # Membrane\n",
    "            dv = dt / tau_mem * (leak - v + i)\n",
    "            v = dv + v\n",
    "\n",
    "            # Spikes\n",
    "            spike = torch.gt(v - threshold, 0.0).to((v - threshold).dtype)\n",
    "            z = spike\n",
    "\n",
    "            # Reset\n",
    "            v = (1 - z.detach()) * v + z.detach() * reset\n",
    "\n",
    "            # Save data\n",
    "            spikes.append(z)\n",
    "            membrane.append(v)\n",
    "\n",
    "        spikes = torch.stack(spikes)\n",
    "        membrane = torch.stack(membrane)\n",
    "        current = torch.stack(current)\n",
    "\n",
    "        ctx.save_for_backward(input, spikes, membrane, current)\n",
    "        ctx.extra_kwargs = {\n",
    "            \"params\": (leak, reset, threshold, tau_syn, tau_mem), \"dt\": dt}\n",
    "\n",
    "        return spikes, membrane, current\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_spikes: torch.Tensor, grad_membrane: torch.Tensor,\n",
    "                grad_current: torch.Tensor) \\\n",
    "            -> Tuple[Optional[torch.Tensor], ...]:\n",
    "        r\"\"\"\n",
    "        Implements 'EventProp' for backward.\n",
    "\n",
    "        :param grad_spikes: Gradient with respect to output spikes.\n",
    "        :param grad_membrane: Gradient with respect to membrane trace.\n",
    "            (Not considered in EventProp algorithm, therefore not used further)\n",
    "        :param grad_current: Gradient with respect to current.\n",
    "            (Not considered in EventProp algorithm, therefore not used further)\n",
    "\n",
    "        :returns: Gradient given by adjoint function lambda_i of current.\n",
    "        \"\"\"\n",
    "\n",
    "        # input and layer data\n",
    "        input_current = ctx.saved_tensors[0][0]\n",
    "        T, _, _ = input_current.shape\n",
    "        z = ctx.saved_tensors[1]\n",
    "        leak, reset, threshold, tau_syn, tau_mem = ctx.extra_kwargs[\"params\"]\n",
    "        dt = ctx.extra_kwargs[\"dt\"]\n",
    "\n",
    "        # adjoints\n",
    "        lambda_v = torch.zeros_like(input_current)\n",
    "        lambda_i = torch.zeros_like(input_current)\n",
    "\n",
    "        if ctx.saved_tensors[3] is not None:\n",
    "            i = ctx.saved_tensors[3]\n",
    "        else:\n",
    "            i = torch.zeros_like(z)\n",
    "            # compute current\n",
    "            for ts in range(T - 1):\n",
    "                i[ts + 1] = \\\n",
    "                    i[ts] * (1 - dt / tau_syn) + input_current[ts]\n",
    "\n",
    "        for ts in range(T - 1, 0, -1):\n",
    "            dv_m = leak - threshold + i[ts - 1]\n",
    "            dv_p = leak - reset + i[ts - 1]\n",
    "\n",
    "            lambda_i[ts - 1] = lambda_i[ts] + dt / \\\n",
    "                tau_syn * (lambda_v[ts] - lambda_i[ts])\n",
    "            lambda_v[ts - 1] = lambda_v[ts] * (1 - dt / tau_mem)\n",
    "\n",
    "            output_term = z[ts] / dv_m * grad_spikes[ts]\n",
    "            output_term[torch.isnan(output_term)] = 0.0\n",
    "            output_term[torch.isinf(output_term)] = 0.0\n",
    "\n",
    "            jump_term = z[ts] * dv_p / dv_m\n",
    "            jump_term[torch.isnan(jump_term)] = 0.0\n",
    "            jump_term[torch.isinf(jump_term)] = 0.0\n",
    "\n",
    "            lambda_v[ts - 1] = (\n",
    "                (1 - z[ts]) * lambda_v[ts - 1]\n",
    "                + jump_term * lambda_v[ts - 1]\n",
    "                + output_term\n",
    "            )\n",
    "\n",
    "        return (torch.stack((lambda_i * tau_syn, lambda_v - lambda_i)),\n",
    "                None, None, None, None, None, None, None)\n",
    "\n",
    "\n",
    "class EventPropSynapseFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Synapse function for proper gradient transport when using EventPropNeuron.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: torch.Tensor, weight: torch.Tensor,\n",
    "                _: torch.Tensor = None\n",
    "                ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        This should be used in combination with EventPropNeuron. Multiply input\n",
    "        with weight and use a stacked output in order to be able to return two\n",
    "        tensors (separate terms in EventProp algorithm), one for previous layer\n",
    "        and the other one for weights.\n",
    "\n",
    "        :param input: Input spikes in shape (batch, time, in_neurons).\n",
    "        :param weight: Weight in shape (out_neurons, in_neurons).\n",
    "        :param _: Bias, which is unused here.\n",
    "\n",
    "        :returns: Returns stacked tensor holding weighted spikes and\n",
    "            tensor with zeros but same shape.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input, weight)\n",
    "        output = input.matmul(weight.t())\n",
    "        return torch.stack((output, torch.zeros_like(output)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor,\n",
    "                ) -> Tuple[Optional[torch.Tensor],\n",
    "                            Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Split gradient_output coming from EventPropNeuron and return\n",
    "        weight * (lambda_v - lambda_i) as input gradient and\n",
    "        - tau_s * lambda_i * input (i.e. - tau_s * lambda_i at spiketimes)\n",
    "        as weight gradient.\n",
    "\n",
    "        :param grad_output: Backpropagated gradient with shape (2, batch, time,\n",
    "            out_neurons). The 2 is due to stacking in forward.\n",
    "\n",
    "        :returns: Returns gradients w.r.t. input, weight and bias (None).\n",
    "        \"\"\"\n",
    "        input, weight = ctx.saved_tensors\n",
    "        grad_input = grad_weight = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output[1].matmul(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = \\\n",
    "                grad_output[0].transpose(0, 1).transpose(1, 2).matmul(\n",
    "                    input.transpose(0, 1))\n",
    "\n",
    "        return grad_input, grad_weight, None\n",
    "\n",
    "\n",
    "class EventPropSynapse(hxsnn.Synapse):\n",
    "    def forward_func(self, input: hxsnn.NeuronHandle) -> hxsnn.SynapseHandle:\n",
    "        return hxsnn.SynapseHandle(\n",
    "            EventPropSynapseFunction.apply(input.spikes, self.weight))\n",
    "\n",
    "\n",
    "class EventPropNeuron(hxsnn.Neuron):\n",
    "    def forward_func(self, input: hxsnn.SynapseHandle,\n",
    "                    hw_data: Optional[Tuple[torch.Tensor]] = None) \\\n",
    "            -> hxsnn.NeuronHandle:\n",
    "        return hxsnn.NeuronHandle(*F.EventPropNeuronFunction.apply(\n",
    "            input.graded_spikes,\n",
    "            self.leak.model_value,\n",
    "            self.reset.model_value,\n",
    "            self.threshold.model_value,\n",
    "            self.tau_syn.model_value,\n",
    "            self.tau_mem.model_value,\n",
    "            hw_data,\n",
    "            self.experiment.dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfed4d7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class EventPropSNN(SNN):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # use EventProp in hidden (spiking) LIF layer\n",
    "        weight = self.linear_h.weight\n",
    "        self.linear_h = EventPropSynapse(\n",
    "            self.n_in * self.input_repetitions,\n",
    "            self.n_hidden,\n",
    "            experiment=self.experiment,\n",
    "            transform=partial(\n",
    "                weight_transforms.linear_saturating, scale=self.weight_scale))\n",
    "        self.linear_h.weight = weight\n",
    "        # Hidden layer\n",
    "        # trace information is not used in EventProp ->  disable cadc recording\n",
    "        # of hidden layer\n",
    "        self.lif_h = EventPropNeuron(\n",
    "            self.n_hidden,\n",
    "            experiment=self.experiment,\n",
    "            tau_mem=self.tau_mem,\n",
    "            tau_syn=self.tau_syn,\n",
    "            leak=hxsnn.MixedHXModelParameter(0., 80),\n",
    "            reset=hxsnn.MixedHXModelParameter(0., 80),\n",
    "            threshold=hxsnn.MixedHXModelParameter(1., 150),\n",
    "            i_synin_gm=500,\n",
    "            synapse_dac_bias=1000,\n",
    "            trace_scale=self.trace_scale,\n",
    "            enable_cadc_recording=False,\n",
    "            cadc_time_shift=self.trace_shift_hidden,\n",
    "            shift_cadc_to_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39f88f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N_HIDDEN      = 120\n",
    "MOCK          = False\n",
    "DT            = 0.5e-06  # s\n",
    "\n",
    "# We need to specify the device we want to use on the host computer\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# The SNN using EventProp functions\n",
    "snn = EventPropSNN(\n",
    "    n_in=5,\n",
    "    n_hidden=N_HIDDEN,\n",
    "    n_out=3,\n",
    "    mock=MOCK,\n",
    "    dt=DT,\n",
    "    tau_mem=6.0e-06,\n",
    "    tau_syn=6.0e-06,\n",
    "    alpha=50.,\n",
    "    trace_shift_hidden=int(.0e-06/DT),\n",
    "    trace_shift_out=int(.0e-06/DT),\n",
    "    weight_init_hidden=(0.15, 0.25),  # higher mean to ensure spiking\n",
    "    weight_init_output=(0.0, 0.1),\n",
    "    weight_scale=66.39,\n",
    "    trace_scale=0.0147,\n",
    "    input_repetitions=1 if MOCK else 5,\n",
    "    device=device)\n",
    "snn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e01f4",
   "metadata": {},
   "source": [
    "#### Training with EventProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e7cae",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "T_SIM   = 3.8e-05  # s\n",
    "T_EARLY = 0.2e-05  # s\n",
    "T_LATE  = 2.6e-05  # s\n",
    "T_BIAS  = 0.2e-05  # s\n",
    "\n",
    "# This encoder translates the points into spikes on a discrete time lattice\n",
    "encoder = CoordinatesToSpikes(\n",
    "    seq_length=int(T_SIM / DT),\n",
    "    t_early=T_EARLY,\n",
    "    t_late=T_LATE,\n",
    "    dt=DT,\n",
    "    t_bias=T_BIAS)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a398e4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model = Model(encoder, snn, decoder, readout_scale=10.)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f6009",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Training params\n",
    "LR            = 0.002\n",
    "STEP_SIZE     = 5\n",
    "GAMMA         = 0.9\n",
    "EPOCHS        = 4 # Adjust here for longer training...\n",
    "BATCH_SIZE    = 50\n",
    "TRAINSET_SIZE = 5000\n",
    "TESTSET_SIZE  = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4f150",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Just for plotting...\n",
    "assert TRAINSET_SIZE % BATCH_SIZE == 0\n",
    "\n",
    "# PyTorch stuff... optimizer, scheduler and loss like you normally do.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Data loaders\n",
    "train_loader, test_loader = get_data_loaders(TRAINSET_SIZE, TESTSET_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae787f30",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Functions to update plot\n",
    "update_plot, update_train_data, update_test_data = plot_training(N_HIDDEN, T_SIM, DT)\n",
    "plt.close()\n",
    "output = w.Output()\n",
    "display(output)\n",
    "\n",
    "# Initialize the hardware\n",
    "if not MOCK:\n",
    "    hxtorch.init_hardware()\n",
    "\n",
    "# Train and test\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "    # Test\n",
    "    loss_test, acc_test, rate_test = test(\n",
    "        model, test_loader, loss, epoch, update_test_data)\n",
    "\n",
    "    # Refresh plot\n",
    "    output.clear_output(wait=True)\n",
    "    with output:\n",
    "        update_plot()\n",
    "\n",
    "    # Train epoch\n",
    "    if epoch < EPOCHS:\n",
    "        loss_train, acc_train = train(\n",
    "            model, train_loader, loss, optimizer, epoch, update_train_data)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# Release the hardware connection\n",
    "hxtorch.release_hardware()"
   ]
  }
 ],
 "metadata": {
  "date": 1741779497.3385525,
  "filename": "ts_05-yin_yang.rst",
  "kernelspec": {
   "display_name": "EBRAINS-experimental",
   "language": "python3",
   "name": "ebrains-experimental"
  },
  "title": "Training an SNN on BrainScaleS-2 with PyTorch"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}