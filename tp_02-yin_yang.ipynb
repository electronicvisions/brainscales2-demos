{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ecfda6c",
   "metadata": {},
   "source": [
    "# Train DNNs on BrainScaleS-2\n",
    "\n",
    "This example uses the PyTorch extension `hxtorch`, already presented in the\n",
    "[introduction to the matrix multiplication](tp_01-properties.ipynb), to train\n",
    "a deep neural network (DNN).\n",
    "\n",
    "In order to use the microscheduler we have to set some environment variables first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb236389",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from _static.common.helpers import setup_hardware_client\n",
    "setup_hardware_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b32d6",
   "metadata": {},
   "source": [
    "Some imports that are needed later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b43e2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets as w\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import hxtorch\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import suppress\n",
    "with suppress(IOError):\n",
    "    plt.style.use(\"_static/matplotlibrc\")\n",
    "%matplotlib inline\n",
    "from IPython.display import HTML\n",
    "display(HTML(\"<style>.output_wrapper button.btn-default, \"\n",
    "             \".output_wrapper .ui-dialog-titlebar {display:none}</style>\"))\n",
    "\n",
    "from _static.common.helpers import save_nightly_calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3a3c3",
   "metadata": {},
   "source": [
    "## Hardware in the loop\n",
    "\n",
    "When training on the BrainScaleS-2 system using hxtorch, the multiply\n",
    "accumulate operation (MAC) in the forward pass is performed on the\n",
    "hardware, but the gradient is computed on the host computer:\n",
    "\n",
    "<img src=\"_static/tutorial/hxtorch_itl.png\" style=\"width:25%;\" align=\"center\">\n",
    "\n",
    "For the calculation of the gradients, a mathematical model is required\n",
    "that approximately reflects the behavior of the hardware operation. In\n",
    "hxtorch, the following very simple linear relationship is assumed for\n",
    "this purpose:\n",
    "\n",
    "$$\n",
    "y_i = \\sum_j x_j \\cdot w_{ij} \\cdot g_\\text{BSS-2} + \\kappa_i\\quad\\text{with}\\quad \\kappa_i \\sim N(0,\\sigma)\n",
    "$$\n",
    "\n",
    "Here, the statistical noise $ \\kappa_i $ of the neurons is assumed\n",
    "to be Gaussian distributed with standard deviation $ \\sigma $. The\n",
    "gain factor $ g_\\text{BSS-2} $ represents the conversion factor\n",
    "between the units of the input, weights and the analog-to-digital\n",
    "converter at the output and is specific to the individual hardware setup\n",
    "and its calibration.\n",
    "\n",
    "For the calculations on the host, these parameters can be measured after\n",
    "initialization of the hardware connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f481a7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# download claibration and initialize hardware configuration\n",
    "save_nightly_calibration('hagen_cocolist.pbin')\n",
    "hxtorch.init_hardware(hxtorch.CalibrationPath('hagen_cocolist.pbin'))\n",
    "\n",
    "# measures the hardware gain and the average statistical noise on the outputs\n",
    "hardware_parameter = hxtorch.perceptron.measure_mock_parameter()\n",
    "\n",
    "print(f\"gain factor: {hardware_parameter.gain:.5f}\")\n",
    "print(f\"noise std.:  {hardware_parameter.noise_std:.5f}\")\n",
    "\n",
    "# use the measured parameters for backward pass and in mock mode\n",
    "hxtorch.perceptron.set_mock_parameter(hardware_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e65e7",
   "metadata": {},
   "source": [
    "## Simulating hardware: The mock mode\n",
    "\n",
    "The linear model of the hardware presented above can optionally also be\n",
    "used for the forward pass with hxtorch. It also features the additional\n",
    "noise, reduced resolution and restricted value ranges of the system.\n",
    "\n",
    "<img src=\"_static/tutorial/hxtorch_mock_mode.png\" style=\"width:90%;\" align=\"center\">\n",
    "\n",
    "This so-called mock mode can be switched on and off individually for\n",
    "each hxtorch operation and for each layer via the `mock` parameter,\n",
    "e.g.\n",
    "\n",
    "`hxtorch.perceptron.matmul(..., mock=True)`\n",
    "\n",
    "It is especially convenient when no BrainScaleS-2 system is available\n",
    "and allows fast prototyping of DNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe4c9d0",
   "metadata": {},
   "source": [
    "## References for further reading\n",
    "\n",
    "The integration into the PyTorch software frontend `hxtorch` and a\n",
    "benchmark on the human activity recognition dataset is published in:\n",
    "\n",
    "- Spilger, Philipp, et al. “hxtorch: PyTorch for BrainScaleS-2.” IoT\n",
    "  Streams for Data-Driven Predictive Maintenance and IoT, Edge, and Mobile\n",
    "  for Embedded Machine Learning. Springer, Cham, 2020. 189-200.\n",
    "  [https://doi.org/10.1007/978-3-030-66770-2_14](https://doi.org/10.1007/978-3-030-66770-2_14)  \n",
    "\n",
    "\n",
    "More details on the implementation of the backward pass, the mock mode\n",
    "and the layer initilization can be found in (chapter 4.2 ff.):\n",
    "\n",
    "- Emmel, Arne “Inference with Convolutional Neural Networks on Analog\n",
    "  Neuromorphic Hardware” *Master’s Thesis*. University of Heidelberg.\n",
    "  [pdf](http://www.kip.uni-heidelberg.de/Veroeffentlichungen/details.php?id=4122)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970cf83d",
   "metadata": {},
   "source": [
    "## Example application: the Yin-Yang dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf2b1f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class YinYangDataset(torch.utils.data.dataset.Dataset):\n",
    "    \"\"\"\n",
    "    The Yin-Yang dataset. Slightly modified version adapted from:\n",
    "    https://github.com/lkriener/yin_yang_data_set\n",
    "    \"\"\"\n",
    "    def __init__(self, r_small=0.1, r_big=0.5, size=1000, seed=42):\n",
    "        super(YinYangDataset, self).__init__()\n",
    "        # numpy RNG to allow compatibility to other learning frameworks\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.r_small = r_small\n",
    "        self.r_big = r_big\n",
    "        self.size = size\n",
    "\n",
    "    def get_sample(self, goal=None):\n",
    "        # sample until goal is satisfied\n",
    "        found_sample_yet = False\n",
    "        while not found_sample_yet:\n",
    "            # sample x,y coordinates\n",
    "            x, y = self.rng.rand(2) * 2. * self.r_big\n",
    "            # check if within yin-yang circle\n",
    "            if np.sqrt((x - self.r_big)**2 + (y - self.r_big)**2) > self.r_big:\n",
    "                continue\n",
    "            # check if they have the same class as the goal for this sample\n",
    "            c = self.which_class(x, y)\n",
    "            if goal is None or c == goal:\n",
    "                found_sample_yet = True\n",
    "                break\n",
    "        return x, y, c\n",
    "\n",
    "    def which_class(self, x, y):\n",
    "        # equations inspired by\n",
    "        # https://link.springer.com/content/pdf/10.1007/11564126_19.pdf\n",
    "        d_right = self.dist_to_right_dot(x, y)\n",
    "        d_left = self.dist_to_left_dot(x, y)\n",
    "        criterion1 = d_right <= self.r_small\n",
    "        criterion2 = d_left > self.r_small and d_left <= 0.5 * self.r_big\n",
    "        criterion3 = y > self.r_big and d_right > 0.5 * self.r_big\n",
    "        is_yin = criterion1 or criterion2 or criterion3\n",
    "        is_circles = d_right < self.r_small or d_left < self.r_small\n",
    "        if is_circles:\n",
    "            return 2\n",
    "        return int(is_yin)\n",
    "\n",
    "    def dist_to_right_dot(self, x, y):\n",
    "        return np.sqrt((x - 1.5 * self.r_big)**2 + (y - self.r_big)**2)\n",
    "\n",
    "    def dist_to_left_dot(self, x, y):\n",
    "        return np.sqrt((x - 0.5 * self.r_big)**2 + (y - self.r_big)**2)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # keep num of class instances balanced by using rejection sampling\n",
    "        # choose class for this sample\n",
    "        goal_class = self.rng.randint(3)\n",
    "        x, y, c = self.get_sample(goal=goal_class)\n",
    "        sample = (torch.tensor([x, y, 1-x, 1-y], dtype=torch.float), c)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98df7c7",
   "metadata": {},
   "source": [
    "Let’s take a look at this dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63505b98",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "colors = ('black', 'white', 'orange')\n",
    "class_names = ('yin', 'yang', 'dot')\n",
    "num_samples = 2000\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=YinYangDataset(size=num_samples),\n",
    "    batch_size=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fed833",
   "metadata": {},
   "source": [
    "The samples in the modified version are randomly redrawn each time they are\n",
    "accessed, so that each sample will be presented to the network only once.\n",
    "Repeated execution of the following code cell therefore shows slightly different\n",
    "samples each time. The number of samples is the same for each of the three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23946b49",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "samples, labels = next(iter(loader))\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "for i in range(3):\n",
    "    ax.scatter(\n",
    "        *samples[labels==i][..., :2].t(),\n",
    "        c=colors[i], label=class_names[i])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_facecolor('gray')\n",
    "ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\")\n",
    "_=ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4318d",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "This dataset as well as some model proposes and benchmarks are presented in:\n",
    "\n",
    "- Kriener, L., Göltz, J., & Petrovici, M. A. (2021). The Yin-Yang dataset.\n",
    "  arXiv preprint: [arXiv:2102.08211](https://arxiv.org/abs/2102.08211).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e6196",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def alpha_cmap(color):\n",
    "    \"\"\"\n",
    "    Returns a colormap ranging from transparent to specified color\n",
    "    \"\"\"\n",
    "    cmap = np.broadcast_to(mpl.colors.to_rgba(color), (256, 4)).copy()\n",
    "    cmap[:, -1] = np.linspace(0, 1, 256)\n",
    "    return mpl.colors.ListedColormap(cmap)\n",
    "\n",
    "def test_train_epoch(model: torch.nn.Module,\n",
    "                     loader: torch.utils.data.DataLoader,\n",
    "                     optimizer: torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Test the model and train for a single epoch afterwards.\n",
    "    :param model: The model\n",
    "    :param loader: Data loader containing the train data set\n",
    "    :param optimizer: Optimizer that handles the weight updates\n",
    "    \"\"\"\n",
    "    # prepare test data (grid of equal spaced samples):\n",
    "    gridsize = 16  # one dimension of the test grid\n",
    "    x = y = torch.linspace(0, 1, gridsize)\n",
    "    x, y = torch.meshgrid(x, y); x, y = x.flatten(), y.flatten()\n",
    "    data_test = torch.tensor(list(zip(x, y, 1-x, 1-y)), requires_grad=True)\n",
    "    data, target = next(iter(loader))\n",
    "    data = torch.cat((data_test, data))  # prepend to train data\n",
    "    # the actual training:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output[len(data_test):], target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # get test data from output and reshape:\n",
    "    with torch.no_grad():\n",
    "        output_test = F.softmax(output[:len(data_test)], dim=-1)\n",
    "    output_test = output_test.reshape(gridsize, gridsize, -1)\n",
    "    return torch.transpose(output_test, 0, 1)\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          loader: torch.utils.data.DataLoader,\n",
    "          scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "          num_epochs: int = 200):\n",
    "    \"\"\"\n",
    "    Train the model while displaying the test results.\n",
    "    :param model: The model\n",
    "    :param loader: Data loader containing the train data set\n",
    "    :param scheduler: Scheduler that handles the weight updates\n",
    "    :param num_epochs: Number of epochs to train\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.grid(False)\n",
    "    ax.set_title(\"Epoch 0\")\n",
    "    imgs = []\n",
    "    for i, c in enumerate(colors):\n",
    "        imgs.append(plt.imshow(\n",
    "            torch.zeros((1, 1)), vmin=0, vmax=1,\n",
    "            extent=(0, 1, 0, 1), origin='lower',\n",
    "            cmap=alpha_cmap(c)))\n",
    "    plt.xlabel(\"$x$\"); plt.ylabel(\"$y$\")\n",
    "\n",
    "    for _ in range(num_epochs + 1):\n",
    "        test_out = test_train_epoch(model, loader, scheduler.optimizer)\n",
    "        for i, img in enumerate(imgs):\n",
    "            img.set_data(test_out[..., i])\n",
    "        ax.set_title(f\"Epoch {scheduler.last_epoch}\")\n",
    "        fig.canvas.draw()\n",
    "        scheduler.step()\n",
    "    wout = w.Output(layout=w.Layout(height=\"450px\")); display(wout)\n",
    "    plt.close(); wout.layout=w.Layout(height=\"0px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b474e",
   "metadata": {},
   "source": [
    "Modeling with hxtorch feels almost like using PyTorch normally, you can\n",
    "even use layers of hxtorch and PyTorch together. If you are familiar\n",
    "with PyTorch, the code below will also look familiar to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6d5e8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Classify the YinYang dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, mock: bool = False):\n",
    "        super().__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            hxtorch.perceptron.nn.Linear(4, 128, mock=mock),\n",
    "            hxtorch.perceptron.nn.ConvertingReLU(shift=1),\n",
    "            hxtorch.perceptron.nn.Linear(128, 3, avg=5, mock=mock),\n",
    "        )\n",
    "\n",
    "    def forward(self, *x):\n",
    "        x = x[0] * 31.  # scale to the whole input range\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac07fc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model_mock = Model(mock=True)\n",
    "model_mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adabf94",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "batch_size = 350\n",
    "lr = 1  # learning rate\n",
    "gamma = 0.99  # learning parameters decay\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=YinYangDataset(size=batch_size),\n",
    "    batch_size=batch_size)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    torch.optim.Adam(model_mock.parameters(), lr=lr),\n",
    "    step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0295774",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "train(model_mock, loader, scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e58a2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "model_hw = Model(mock=False)\n",
    "# initialize with state of mock model\n",
    "model_hw.load_state_dict(model_mock.state_dict())\n",
    "model_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e9843",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lr = 0.2\n",
    "gamma = 0.9\n",
    "\n",
    "scheduler_hw = torch.optim.lr_scheduler.StepLR(\n",
    "    torch.optim.Adam(model_hw.parameters(), lr=lr),\n",
    "    step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7e23b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "train(model_hw, loader, scheduler_hw, num_epochs=15)"
   ]
  }
 ],
 "metadata": {
  "date": 1734550620.9796536,
  "filename": "tp_02-yin_yang.rst",
  "kernelspec": {
   "display_name": "EBRAINS-experimental",
   "language": "python3",
   "name": "ebrains-experimental"
  },
  "title": "Train DNNs on BrainScaleS-2"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}