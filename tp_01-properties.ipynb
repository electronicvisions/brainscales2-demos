{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dffb1941",
   "metadata": {},
   "source": [
    "# Exploring the analog MAC operation\n",
    "\n",
    "This example presents the non-spiking mode of the BrainScaleS-2 ASIC and\n",
    "some of its characteristics. The operation of this so-called hagen mode\n",
    "is explained in more detail in the matrix multiplication introduction.\n",
    "\n",
    "In order to use the microscheduler we have to set some environment variables first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d2314",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from _static.common.helpers import setup_hardware_client\n",
    "setup_hardware_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368fc950",
   "metadata": {},
   "source": [
    "First, we import some things needed later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3336da",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import hxtorch\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import suppress\n",
    "with suppress(IOError):\n",
    "    plt.style.use(\"_static/matplotlibrc\")\n",
    "\n",
    "from _static.common.helpers import save_nightly_calibration\n",
    "\n",
    "import ipywidgets as w\n",
    "from functools import partial\n",
    "IntSlider = partial(w.IntSlider, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad3ffd",
   "metadata": {},
   "source": [
    "## The `hxtorch` API\n",
    "\n",
    "The hagen mode provides an analog multiply accumulate operation (MAC)\n",
    "which is performed on the ASIC.\n",
    "\n",
    "**hxtorch** provides a high-level API for this operation mode that\n",
    "integrates this functionality into [PyTorch](https://pytorch.org/).\n",
    "In analogy to some functions of this  machine-learning framework,\n",
    "operations with similar API are provided, e.g. `matmul` for\n",
    "multiplication of two matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba999ab",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print(hxtorch.perceptron.matmul.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df478f02",
   "metadata": {},
   "source": [
    "````Before the hardware can be used, we have to allocate a connection and to\n",
    "load a calibration. This can be achieved using `hxtorch.init_hardware`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b66e5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# download claibration and initialize hardware configuration\n",
    "save_nightly_calibration('hagen_cocolist.pbin')\n",
    "hxtorch.init_hardware(hxtorch.CalibrationPath('hagen_cocolist.pbin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a9143",
   "metadata": {},
   "source": [
    "This already enables us to multiply matrices using the BSS-2 accelerator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ea1c2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "M1 = torch.full((100,), 15.)\n",
    "M2 = torch.full((100, 10), 21.)\n",
    "hxtorch.perceptron.matmul(M1, M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6ea24",
   "metadata": {},
   "source": [
    "`hxtorch` integrates the MAC operation into PyTorch on a per-operation\n",
    "basis (but also supports the combination of multiple operations) and is\n",
    "executed just-in-time on the BrainScaleS-2 hardware.\n",
    "\n",
    "<img src=\"_static/tutorial/hxtorch_matmul.png\" style=\"width:80%;\" align=\"center\">\n",
    "\n",
    "A decisive advantage of the matrix multiplication mode is the possibility\n",
    "to decompose large operations and smaller parts and either multiplex them\n",
    "in time or even divide them among several BrainScaleS-2 ASICs:\n",
    "\n",
    "<img src=\"_static/tutorial/hxtorch_partitioning.png\" style=\"width:80%;\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac0dd9",
   "metadata": {},
   "source": [
    "## Noise and fixed-pattern deviations\n",
    "\n",
    "Despite calibration and even with the same inputs and weights, the\n",
    "outputs of the different neurons are not identical. On the one hand,\n",
    "each output has a statistical noise due to the analog nature of the\n",
    "neuron, on the other hand, fixed-pattern deviations show up between the\n",
    "individual neurons. Especially in the case of small inputs, a spatial\n",
    "correlation may also become apparent, resulting from different distances\n",
    "to the synapse drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f62d2f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# prepare output figure\n",
    "neurons = torch.arange(0, 256)\n",
    "slices = [slice(0, 128), slice(128, 256)]\n",
    "fig, axes = plt.subplots(1, 2, sharey=True)\n",
    "for ax, s in zip(axes, slices):\n",
    "    ax.plot(neurons[s], torch.zeros_like(neurons[s]), \".\", c=\"C0\")\n",
    "    ax.set_xlim(s.start, s.stop); ax.set_ylim(-130, 130)\n",
    "    ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(32))\n",
    "    ax.set_xlabel(\"neuron #\"); ax.set_ylabel(\"output\"); ax.label_outer()\n",
    "axes[0]; axes[0].invert_xaxis()\n",
    "plt.close()\n",
    "output = w.Output()\n",
    "\n",
    "@w.interact(\n",
    "    num_sends=IntSlider(100, 1, 256, description=\"num sends\"),\n",
    "    input_value=IntSlider(12, 0, 31, description=\"input value\"),\n",
    "    weight_value=IntSlider(21, -63, 63, description=\"weight value\"),\n",
    "    row_number=IntSlider(0, 0, 127, description=\"row number\"),\n",
    ")\n",
    "def experiment(num_sends, input_value, weight_value, row_number):\n",
    "    \"\"\" Updates the plot with the outputs from the hardware \"\"\"\n",
    "    result = hxtorch.perceptron.matmul(\n",
    "        torch.tensor([0.] * row_number + [input_value], dtype=torch.float),\n",
    "        torch.full((row_number + 1, 256), weight_value, dtype=torch.float),\n",
    "        num_sends=num_sends)\n",
    "    for ax, s in zip(axes, slices):\n",
    "        ax.lines[0].set_ydata(result[s])\n",
    "    output.clear_output(wait=True)\n",
    "    with output:\n",
    "        display(fig)\n",
    "experiment(100, 12, 21, 0)  # needed for testing\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad25a43",
   "metadata": {},
   "source": [
    "## Linearity of the MAC operation\n",
    "\n",
    "The next plot shows the linear relationship between input, weight and\n",
    "output. For this purpose, a constant input is multiplied by a linearly\n",
    "increasing weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d12c077",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "weight = torch.arange(-63, 64.).repeat_interleave(2)\n",
    "\n",
    "# prepare output figure\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(weight, torch.zeros_like(weight), \".\", c=\"C0\")\n",
    "ax.set_xlim(-64, 64); ax.set_ylim(-130, 130)\n",
    "ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(16))\n",
    "ax.set_xlabel(\"weight\"); ax.set_ylabel(\"output\")\n",
    "plt.close()\n",
    "output = w.Output()\n",
    "\n",
    "@w.interact(\n",
    "    num_sends=IntSlider(100, 1, 256, description=\"num sends\"),\n",
    "    input_value=IntSlider(12, 0, 31, description=\"input value\"),\n",
    "    row_number=IntSlider(0, 0, 127, description=\"row number\"),\n",
    ")\n",
    "def experiment(num_sends, input_value, row_number):\n",
    "    \"\"\" Updates the plot with the outputs from the hardware \"\"\"\n",
    "    result = hxtorch.perceptron.matmul(\n",
    "        torch.tensor([0.] * row_number + [input_value], dtype=torch.float),\n",
    "        weight.unsqueeze(0).expand(row_number + 1, -1),\n",
    "        num_sends=num_sends)\n",
    "    ax.lines[0].set_ydata(result)\n",
    "    output.clear_output(wait=True)\n",
    "    with output:\n",
    "        display(fig)\n",
    "experiment(100, 12, 0)  # needed for testing\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a10015",
   "metadata": {},
   "source": [
    "At output values of about -80 to 80 a good linear correlation can be\n",
    "observed. For smaller or larger values, the used ADC saturates; this\n",
    "happens earlier for some neurons and later for others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f2aa79",
   "metadata": {},
   "source": [
    "### Possible questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2031cb",
   "metadata": {},
   "source": [
    "#### How does the result change with several successive calls to `hxtorch.perceptron.matmul`?\n",
    "\n",
    "Due to its analog nature, the BrainScaleS-2 ASIC provides slightly\n",
    "different values for each call. Quantify the noise on each neuron!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee3a693",
   "metadata": {},
   "source": [
    "#### What is the relationship between input and output? Is it linear?\n",
    "\n",
    "We have seen that the relationship between weight and output is quite\n",
    "linear at intermediate values. How, on the other hand, does the output\n",
    "change with changing inputs and constant weight? Is the relationship\n",
    "linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b122b7",
   "metadata": {},
   "source": [
    "#### Negative inputs?\n",
    "\n",
    "The inputs to the multiply accumulate operation correspond to the time a\n",
    "current flows on neuron membranes, which means they must be positive\n",
    "only. How would it still be possible to allow negative inputs in a\n",
    "calculation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f4c1c",
   "metadata": {},
   "source": [
    "The integration with PyTorch allows the MAC to be used very easily for\n",
    "conventional machine learning. For this, the forward pass is computed with\n",
    "the ASIC, the backward pass on the host computer. The example for training\n",
    "DNNs shows such a usage."
   ]
  }
 ],
 "metadata": {
  "date": 1740990468.0327766,
  "filename": "tp_01-properties.rst",
  "kernelspec": {
   "display_name": "EBRAINS-experimental",
   "language": "python",
   "name": "ebrains-experimental"
  },
  "title": "Exploring the analog MAC operation"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}