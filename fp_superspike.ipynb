{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108250bd",
   "metadata": {},
   "source": [
    "# Learning with the SuperSpike rule\n",
    "\n",
    "In the previous example, you accustomed yourself with BrainScaleS-2’s\n",
    "analog neuron circuit. Here, we will build on this knowledge and train a\n",
    "single neuron to reproduce a pre-defined target spike train. For this\n",
    "purpose, we will implement the SuperSpike learning rule [1] to minimize\n",
    "the van Rossum distance between the targets and the spikes produced by\n",
    "our neuron.\n",
    "\n",
    "Specifically, we will\n",
    "\n",
    "- set up a network using PyNN,  \n",
    "- generate a target spike train,  \n",
    "- re-initialize the network’s weights,  \n",
    "- and implement an in-the-loop training scheme directly in Python.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a7e27",
   "metadata": {},
   "source": [
    "## Training spiking networks with surrogate gradients\n",
    "\n",
    "The discontinuity introduced by the spiking threshold presents\n",
    "challenges when it comes to training networks of spiking neurons with\n",
    "gradient-based methods, especially when employing sparse coding\n",
    "techniques far away from the rate coding limit. Recently, efforts have\n",
    "been made by “smoothing” out the true activation function for the\n",
    "backward pass [1,2,3,4,5]. These “surrogate gradients” allow to easily\n",
    "train feed-forward as well as recurrent SNNs on common benchmark datasets.\n",
    "\n",
    "For BrainScaleS-2, we have developed a powerful in-the-loop training\n",
    "scheme that allows to perform backpropagation through time using\n",
    "auto-differentiation frameworks [6]. With a feed-forward network, we\n",
    "reached 97.6 % test accuracy on the MNIST dataset. The accelerated\n",
    "emulation allowed to classify more than 85k images per second with an energy\n",
    "budget of only 2.4 μJ per image. Moreover, we trained a recurrent SNN on\n",
    "natural language data from the SHD dataset [7].\n",
    "\n",
    "In this example, we will not go that far. Instead, we will implement the\n",
    "forward-integrated SuperSpike learning rule on a simple artifical input\n",
    "pattern. In case you are interested, though, please refer to the original\n",
    "publication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176c7a6",
   "metadata": {},
   "source": [
    "## References for further reading\n",
    "\n",
    "1. Zenke, Friedemann, and Surya Ganguli. “Superspike: Supervised\n",
    "  learning in multilayer spiking neural networks.” *Neural computation* 30.6 (2018): 1514-1541.  \n",
    "1. Neftci, Emre O., Hesham Mostafa, and Friedemann Zenke. “Surrogate\n",
    "  gradient learning in spiking neural networks: Bringing the power of\n",
    "  gradient-based optimization to spiking neural networks.” IEEE Signal\n",
    "  Processing Magazine 36.6 (2019): 51-63.  \n",
    "1. Bellec, Guillaume, et al. “A solution to the learning dilemma for\n",
    "  recurrent networks of spiking neurons.” Nature communications 11.1\n",
    "  (2020): 1-15.  \n",
    "1. Bohte, Sander M. “Error-backpropagation in networks of fractionally\n",
    "  predictive spiking neurons.” International Conference on Artificial\n",
    "  Neural Networks. Springer, Berlin, Heidelberg, 2011.  \n",
    "1. Esser, Steven K., et al. “Convolutional networks for fast,\n",
    "  energy-efficient neuromorphic computing.” Proceedings of the national\n",
    "  academy of sciences 113.41 (2016): 11441-11446.  \n",
    "1. Cramer, Benjamin, et al. “Surrogate gradients for analog neuromorphic\n",
    "  computing” Proceedings of the national academy of sciences 119.4 (2022)  \n",
    "1. Cramer, Benjamin, et al. “The Heidelberg Spiking Data Sets for the\n",
    "  Systematic Evaluation of Spiking Neural Networks.” IEEE Transactions\n",
    "  on Neural Networks and Learning Systems (2020).  \n",
    "\n",
    "\n",
    "In order to use the microscheduler we have to set some environment variables first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18488e2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from _static.common.helpers import setup_hardware_client\n",
    "setup_hardware_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1272cf",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import ipywidgets as w\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import suppress\n",
    "with suppress(IOError):\n",
    "    plt.style.use(\"_static/matplotlibrc\")\n",
    "\n",
    "import pynn_brainscales.brainscales2 as pynn\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53aaf3",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Let’s define some helper functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede9565",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def create_figure():\n",
    "    fig, axes = plt.subplots(2, 1)\n",
    "\n",
    "    axes[0].set_ylabel(\"norm. membrane\")\n",
    "    axes[0].set_xlim(0, duration * 1e6)\n",
    "    axes[0].set_xticklabels([])\n",
    "    axes[0].plot([], [], c=\"C0\")\n",
    "    axes[0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "    axes[1].set_xlabel(\"time / μs\")\n",
    "    axes[1].set_ylabel(\"error trace\")\n",
    "    axes[1].set_xlim(0, duration * 1e6)\n",
    "    axes[1].plot([], [], c=\"C0\")\n",
    "    axes[1].set_ylim(-0.5, 0.5)\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "def update_figure(fig, *data):\n",
    "    for ax, d in zip(fig.axes, data):\n",
    "        ax.get_lines()[0].set_data(d[0] * 1e6, d[1])\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810f795",
   "metadata": {},
   "source": [
    "## Define network in PyNN\n",
    "\n",
    "First, we will set up some variables determining the network structure. We will\n",
    "furthermore define the binning of the stimuli and the later calculation of the\n",
    "weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71c7b4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n_inputs = 25\n",
    "\n",
    "duration = 200e-6  # s in wallclock time\n",
    "dt = 0.1e-6\n",
    "\n",
    "n_steps = int(duration / dt)\n",
    "\n",
    "time = np.arange(n_steps) * dt\n",
    "bins = np.arange(n_steps + 1) * dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab664c",
   "metadata": {},
   "source": [
    "Before we define our network, we load the default calibration.\n",
    "\n",
    "A default calibration is generated for every setup every night.\n",
    "We save the nightly calibration in two variables such that we can use it later when we define our neuronal network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47995e82",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from _static.common.helpers import get_nightly_calibration\n",
    "calib = get_nightly_calibration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebeb437",
   "metadata": {},
   "source": [
    "Now, we can define the network itself using PyNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccaa675",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# setup PyNN and inect calibration data\n",
    "pynn.setup(initial_config=calib)\n",
    "\n",
    "# create output population (one neuron) and record its observables\n",
    "pop_output = pynn.Population(1, pynn.cells.HXNeuron())\n",
    "pop_output.record([\"spikes\", \"v\"])\n",
    "\n",
    "# create spike sources\n",
    "pop_input = pynn.Population(n_inputs, pynn.cells.SpikeSourceArray(spike_times=[]))\n",
    "\n",
    "# define two projections (excitatory + inhibitory) to allow signed weights\n",
    "synapse_exc = pynn.standardmodels.synapses.StaticSynapse(weight=42)\n",
    "synapse_inh = pynn.standardmodels.synapses.StaticSynapse(weight=-42)\n",
    "projection_io_inh = pynn.Projection(pop_input, pop_output,\n",
    "                             pynn.AllToAllConnector(),\n",
    "                             synapse_type=synapse_inh,\n",
    "                             receptor_type=\"inhibitory\")\n",
    "projection_io_exc = pynn.Projection(pop_input, pop_output,\n",
    "                             pynn.AllToAllConnector(),\n",
    "                             synapse_type=synapse_exc,\n",
    "                             receptor_type=\"excitatory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a84cc",
   "metadata": {},
   "source": [
    "To work around Dale’s law, we have to merge two projections together to\n",
    "form signed synapses. The following function assigns the signed weight\n",
    "matrix to the two projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fbedce",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def set_weights(weights, w_max=63):\n",
    "    # limit weights to hw boundaries\n",
    "    weights = weights.clip(-w_max, w_max)\n",
    "\n",
    "    integer_weights = np.round(weights).astype(int)\n",
    "    w_exc = integer_weights * (integer_weights >= 0).astype(int)\n",
    "    w_inh = integer_weights * (integer_weights < 0).astype(int)\n",
    "\n",
    "    projection_io_inh.set(weight=w_inh)\n",
    "    projection_io_exc.set(weight=w_exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49724f",
   "metadata": {},
   "source": [
    "## Construct Poisson input spike trains\n",
    "\n",
    "To generate (fixed-seed) random inputs, we calculate binned spike trains\n",
    "according to a Bernoulli process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c520a3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "\n",
    "freq = 10e3  # Hz (remember the acceleration factor!)\n",
    "input_spikes = []\n",
    "stimuli_dense = np.random.rand(n_inputs, n_steps) < freq * dt\n",
    "stimuli_dense[:, (time > (duration - 20e-6))] = 0\n",
    "\n",
    "for s in stimuli_dense:\n",
    "    input_spikes.append(np.where(s)[0] * dt * 1e3) # convert to ms for pyNN\n",
    "\n",
    "pop_input.set(spike_times=input_spikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d531ef48",
   "metadata": {},
   "source": [
    "## Generating a taget spike train\n",
    "\n",
    "Now, we can inject the previously defined input spike trains into our target\n",
    "neuron. For this purpose, we first randomly initialize the synaptic weights.\n",
    "\n",
    "The resulting output spikes will later be used as a target spike train.\n",
    "The difficulty of the task will depend on the number and timing of target\n",
    "spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a13ad5",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- Play around with the mean and standard deviation of the weight distribution\n",
    "  and observe the output of the neuron. Try to get the neuron to emit\n",
    "  approximately 3 to 4 spikes. This spike train will later be used as a target\n",
    "  spike train $ \\hat S_i $.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0dcc86",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@w.interact(\n",
    "    weight_mean=w.FloatSlider(\n",
    "        27, min=0, max=40, continuous_update=False),\n",
    "    weight_std=w.FloatSlider(\n",
    "        1, min=0, max=10, continuous_update=False)\n",
    ")\n",
    "def experiment(weight_mean, weight_std):\n",
    "    global v_mem, target_spikes\n",
    "    np.random.seed(123)\n",
    "    weights = np.random.normal(weight_mean, weight_std, size=(n_inputs, 1))\n",
    "    set_weights(weights)\n",
    "    pynn.run(duration * 1e3)\n",
    "\n",
    "    data = pop_output.get_data()\n",
    "\n",
    "    target_spikes = data.segments[-1].spiketrains[0] / 1e3  # convert ms to s\n",
    "\n",
    "    membrane = data.segments[-1].irregularlysampledsignals[0]\n",
    "    v_mem = np.interp(time, membrane.times / 1e3, membrane.magnitude[:, 0])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.plot(time*1e6, v_mem)\n",
    "\n",
    "    ax.set_xlim(0, duration*1e6)\n",
    "    ax.set_xlabel(\"time / μs\")\n",
    "    ax.set_ylabel(\"membrane potential / LSB\")\n",
    "    pynn.reset()\n",
    "experiment(27, 1); plt.close()  # needed for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9929765",
   "metadata": {},
   "source": [
    "Extract the dynamic range from the above plot to normalize the membrane\n",
    "potential for calculating the surrogate gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1d441",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "v_zero = v_mem.min()\n",
    "dynamic_range = v_mem.max() - v_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc90827",
   "metadata": {},
   "source": [
    "## The SuperSpike learning rule\n",
    "\n",
    "The SuperSpike learning rule was derived to perform gradient descent on\n",
    "the van Rossum distance\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} \\int_{-\\infty}^t \\mathrm{d}s \\, \\left[ \\left( \\alpha * \\hat S_i(s) - \\alpha * S_i(s) \\right) \\right]^2 =: \\frac{1}{2} \\int_{-\\infty}^t \\mathrm{d}s \\, e_i(s)^2\n",
    "$$\n",
    "\n",
    "between the current spike train $ S_i $ and the target spike train $ \\hat S_i $.\n",
    "Here, $ \\alpha $ is the kernel used to calculate the van Rossum distance and $ e_i(s) $ the error signal.\n",
    "The weight update rule can in the end be written as\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij}^k = \\eta \\int \\mathrm{d}s \\, e_i(s) \\cdot \\alpha * \\left[ \\sigma^\\prime (v_i(s)) \\cdot (\\epsilon * S_j)(s) \\right] \\,\n",
    "$$\n",
    "\n",
    "where $ \\sigma^\\prime $ represents the\n",
    "surrogate gradient of membrane potential $ v_i $, and\n",
    "$ \\epsilon $ the exponentially decaying kernel of the synaptic\n",
    "currents.\n",
    "\n",
    "The integral consists of a Hebbian contribution which combines the\n",
    "surrogate gradient of the membrane potential with the exponentially\n",
    "decaying synaptic currents as eligibility traces. This term is augmented\n",
    "by the error signal as a third factor, which can be calculated through\n",
    "backpropagation for multi-layer networks.\n",
    "\n",
    "The learning rule can be forward-integrated alongside the neuronal\n",
    "dynamics, which makes it particularly interesting for online learning\n",
    "applications.\n",
    "\n",
    "Let’s have a look at the surrogate function $ \\sigma^\\prime $ as a function of\n",
    "the steepness paramter $ \\beta $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80625d88",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def superspike(v_m, beta=5.0):\n",
    "    return np.power(1 + np.abs(beta * (v_m - 1.0)), -2)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "v = np.linspace(0, 1, 100)\n",
    "\n",
    "for beta in np.logspace(np.log10(1), np.log10(10), 4):\n",
    "    ax.plot(v, superspike(v, beta=beta), label=f\"beta = {beta:.1f}\")\n",
    "\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_xlabel(\"normalized membrane potential\")\n",
    "ax.set_ylabel(\"surrogate gradient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a736168",
   "metadata": {},
   "source": [
    "The SuperSpike learning rules requires estimates of the neuro-synaptic\n",
    "time constants. Here, we use the same values as targeted for the deployed\n",
    "calibration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a5cd6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "tau_mem = 10e-6\n",
    "tau_syn = 5e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dedb1f",
   "metadata": {},
   "source": [
    "Construct kernels for the learning rule, including the van Rossum\n",
    "distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0d7e7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n_kernel_steps = int(100e-6 / dt)\n",
    "n_kernel_steps = n_kernel_steps + (1 - n_kernel_steps % 2)\n",
    "\n",
    "kernel_psc = np.zeros(n_kernel_steps)\n",
    "kernel_psc[-int(kernel_psc.size / 2):] += np.exp(-np.arange(int(kernel_psc.size / 2)) / (tau_syn / dt))\n",
    "\n",
    "kernel_psp = kernel_psc.copy()\n",
    "kernel_psp[-int(kernel_psp.size / 2):] -= np.exp(-np.arange(int(kernel_psp.size / 2)) / (tau_mem / dt))\n",
    "\n",
    "kernel_vrd = kernel_psp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a47e9",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "We can now implement above’s weight update expression in Python and use it\n",
    "to train our network to replicate the target spike train generated above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d3b070",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- Train the network for different target spike trains ($ \\hat S_i $). For that purpose,\n",
    "  modify above’s cell for the target generation (e.g. seed, firing rate,\n",
    "  weights, …).  \n",
    "- Play around with the hyper parameters such as the learning rate (eta).  \n",
    "- How does the steepness of the surrogate gradient (beta) affect learning\n",
    "  performance?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5389d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, axes = create_figure()\n",
    "plt.close()\n",
    "output = w.Output()\n",
    "display(output)\n",
    "\n",
    "# plot target spikes\n",
    "for t in target_spikes:\n",
    "    axes[0].axvline(t * 1e6, c=\"orange\", zorder=20)\n",
    "\n",
    "# define hyperparameters\n",
    "n_epochs = 100\n",
    "eta = 20.\n",
    "beta = 5.\n",
    "\n",
    "# initialize weights\n",
    "weights = np.random.normal(10, 20, size=(n_inputs, 1))\n",
    "\n",
    "# iterate over multiple training \"epochs\"\n",
    "loss = np.zeros(n_epochs)\n",
    "for e in range(n_epochs):\n",
    "    # assign weights to PyNN projections\n",
    "    set_weights(weights)\n",
    "\n",
    "    # run the emulation\n",
    "    pynn.run(duration * 1e3) # convert to ms for PyNN\n",
    "\n",
    "    # retrieve data\n",
    "    data = pop_output.get_data()\n",
    "    spikes = data.segments[-1].spiketrains[0] / 1e3 # convert to SI units (s)\n",
    "    membrane = data.segments[-1].irregularlysampledsignals[0]\n",
    "\n",
    "    # resample and normalize mebrane trace\n",
    "    v_mem = (np.interp(time, membrane.times / 1e3, membrane.magnitude[:, 0]) - v_zero) / dynamic_range\n",
    "\n",
    "    # reset pyNN state\n",
    "    pynn.reset()\n",
    "\n",
    "    # compute van-Rossum distance as error signal\n",
    "    error = np.convolve(\n",
    "            np.histogram(target_spikes, bins)[0] - np.histogram(spikes, bins)[0],\n",
    "            kernel_vrd, mode=\"same\")\n",
    "\n",
    "    output.clear_output(wait=True)\n",
    "    with output:\n",
    "        update_figure(fig, (time, v_mem), (time, error))\n",
    "\n",
    "    # calculate weight updates\n",
    "    dw = np.zeros((n_inputs, 1))\n",
    "    for source in range(n_inputs):\n",
    "        eligibility = np.convolve(stimuli_dense[source, :], kernel_psc, mode=\"same\")\n",
    "        integrand = error * np.convolve(\n",
    "            superspike(v_mem, beta=beta) * eligibility,\n",
    "            kernel_psp,\n",
    "            mode=\"same\")\n",
    "        dw[source, 0] = eta * np.sum(integrand) / n_steps\n",
    "\n",
    "    # save the loss for later plotting\n",
    "    loss[e] = np.sum(np.abs(error))\n",
    "\n",
    "    # apply weight update\n",
    "    weights += dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c043ef",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.plot(loss)\n",
    "\n",
    "ax.set_xlim(0, n_epochs)\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\");"
   ]
  }
 ],
 "metadata": {
  "date": 1728385271.2881265,
  "filename": "fp_superspike.rst",
  "kernelspec": {
   "display_name": "EBRAINS-experimental",
   "language": "python",
   "name": "ebrains-experimental"
  },
  "title": "Learning with the SuperSpike rule"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}