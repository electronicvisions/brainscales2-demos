{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13ca496",
   "metadata": {},
   "source": [
    "# hxtorch.snn Introduction\n",
    "\n",
    "In this tutorial we will explore the `hxtorch.snn` framework used to train networks of spiking neurons on the BrainScaleS-2 platform in a machine-learning-inspired fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e97b6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from _static.tutorial.hxsnn_intro_plots import plot_training, plot_compare_traces, plot_mock, plot_scaled_trace, plot_targets\n",
    "from _static.common.helpers import setup_hardware_client, save_nightly_calibration\n",
    "setup_hardware_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6221c0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import hxtorch\n",
    "import hxtorch.snn as hxsnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501709f3",
   "metadata": {},
   "source": [
    "## Emulate a network on BSS-2\n",
    "\n",
    "To start, we create a small network with a single spiking leak-integrate and fire (LIF) `Neuron`, receiving inputs through a linear `Synapse` layer.\n",
    "Network layers in `hxtorch` are derived from a parent class `HXModule` (similar to `torch.nn.Module` in PyTorch), requiring an instance of `Experiment` which keeps track of all network layers to be run within the same experiment on BSS-2.\n",
    "While `Neuron`s can be parameterized individually, we keep the default parameters for now.\n",
    "\n",
    "Next we create some input spikes and emulate the network.\n",
    "The membrane potential of the LIF is measured using the columnar ADC (CADC) and the membrane (MADC).\n",
    "While the first allows sampling all neuron potentials on the chip in parallel, the latter can only sample two neurons at the benefit of having a higher temporal resolution.\n",
    "The units are CADC / MADC Values, which in principle are translatable to mV (which will be implemented in the future).\n",
    "When modules are called to infere the networks topology, they return `Handles` (e.g. `g`, `z`) which act as promises to future hardware data which get filled after the hardware experiment is run.\n",
    "Note that in `hxtorch` the returned hardware data is mapped to a dense time grid of shape `(batch_size, time_steps, population_size)` of resolution `dt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fe230",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "save_nightly_calibration('spiking_calix-native.pkl')\n",
    "\n",
    "hxtorch.init_hardware()\n",
    "\n",
    "# Experiment\n",
    "exp = hxsnn.Experiment(dt=1e-6, calib_path='spiking_cocolist.pbin')\n",
    "\n",
    "# Modules\n",
    "syn = hxsnn.Synapse(\n",
    "    in_features=1,\n",
    "    out_features=1,\n",
    "    experiment=exp)\n",
    "lif = hxsnn.Neuron(\n",
    "    size=1,\n",
    "    experiment=exp,\n",
    "    enable_cadc_recording=True,\n",
    "    enable_spike_recording=True,\n",
    "    cadc_time_shift=-1)\n",
    "\n",
    "# Weights on hardware are between -63 to 63\n",
    "syn.weight.data.fill_(63)\n",
    "\n",
    "# Some random input spikes\n",
    "inputs = torch.zeros((50, 1, 1))\n",
    "inputs[[10, 15, 20, 30]] = 1  # in dt\n",
    "\n",
    "# Forward\n",
    "g = syn(hxsnn.NeuronHandle(inputs))\n",
    "z = lif(g)\n",
    "\n",
    "print(z.spikes, z.v_cadc)\n",
    "\n",
    "hxsnn.run(exp, 50)  # dt\n",
    "\n",
    "print(z.spikes.shape, z.v_cadc.shape)\n",
    "\n",
    "# Display\n",
    "plot_compare_traces(inputs, z)\n",
    "\n",
    "hxtorch.release_hardware()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc6642",
   "metadata": {},
   "source": [
    "Great, we have now seen how easily spiking neural networks are emulated on BSS-2.\n",
    "In order to train them, each layer instance of $ HXModule $ has a PyTorch-differentiable numerical representation defined in a member function `forward_func`.\n",
    "This function allows backpropagating gradients but also to simulate networks.\n",
    "Simulation is enabled by setting `mock=True` in the `Experiment` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f4fd7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Experiment\n",
    "exp = hxsnn.Experiment(dt=1e-6, mock=True)\n",
    "\n",
    "# Modules\n",
    "syn = hxsnn.Synapse(1, 1, exp, func=hxsnn.functional.linear)\n",
    "syn.weight.data.fill_(50)  # weights are between -63 to 63\n",
    "lif = hxsnn.Neuron(\n",
    "    1, exp,\n",
    "    params=hxsnn.functional.CUBALIFParams(1./10e-6, 1./10e-6, v_leak=80, v_reset=80, v_th=125),\n",
    "    func=hxsnn.functional.cuba_lif_integration)\n",
    "\n",
    "# Forward\n",
    "g = syn(hxsnn.NeuronHandle(inputs))\n",
    "z = lif(g)\n",
    "\n",
    "# Simulate\n",
    "hxsnn.run(exp, 50)  # dt\n",
    "\n",
    "plot_mock(inputs, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a91ae6",
   "metadata": {},
   "source": [
    "Since the dynamics of the numerics is used to compute gradients it is important to align them to the dynamics of the neurons on hardware.\n",
    "If you compare the y-axes between the first and the second plot you see, that this is not the case here.\n",
    "There are two scalings which can be used to align the dynamics.\n",
    "First there is the `trace_scaling` which is applied to the measured membrane trace, this allows to scale the hardware values into the expected simulated range.\n",
    "Additionally, the measured traces can be offset either by setting `shift_to_fist=True` which uses the first measured value as baseline, or setting a `trace_offset` specifically.\n",
    "The second scaling is the scaling between the weights used in the sofware model and the weights used on hardware, since the “strength” of the weights on BSS-2 depend on the calibration.\n",
    "\n",
    "First we want to align the membrane ranges.\n",
    "For this, we assume `leak=0`, `reset=0` and `threshold=1` for the LIF neuron in the numerics.\n",
    "`Neuron`s can be parameterized with different values for the `model` in software which is used for gradient computation and the neuron on `hardware` using a `MixedHXModelParameter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da71fe",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from hxtorch.snn.transforms.weight_transforms import linear_saturating\n",
    "\n",
    "hxtorch.init_hardware()\n",
    "\n",
    "# Model parameters\n",
    "model_threshold = 1.\n",
    "model_leak = 0.\n",
    "model_reset = 0.\n",
    "\n",
    "# BSS-2 parameters\n",
    "bss2_threshold = 125\n",
    "bss2_leak = 80\n",
    "bss2_reset = 80\n",
    "\n",
    "# Post-processing\n",
    "# These values are measured in the following\n",
    "trace_scale = 1.\n",
    "trace_offset = 0.\n",
    "weight_scale = 1.\n",
    "\n",
    "\n",
    "def run(inputs, mock=False, weight_scale=63., trace_offset=0., trace_scale=1., n_runs=10):\n",
    "    \"\"\" \"\"\"\n",
    "    traces = []\n",
    "    for i in range(n_runs):\n",
    "        # Experiment\n",
    "        exp = hxsnn.Experiment(dt=1e-6, mock=mock, calib_path='spiking_cocolist.pbin')\n",
    "\n",
    "        # Modules\n",
    "        syn = hxsnn.Synapse(\n",
    "            in_features=1,\n",
    "            out_features=1,\n",
    "            experiment=exp,\n",
    "            transform=partial(linear_saturating, scale=weight_scale))\n",
    "        lif = hxsnn.Neuron(\n",
    "            size=1,\n",
    "            experiment=exp,\n",
    "            params=hxsnn.functional.CUBALIFParams(1./10e-6, 1./10e-6, v_leak=0, v_reset=0, v_th=1),\n",
    "            func=hxsnn.functional.cuba_lif_integration,\n",
    "            cadc_time_shift=-1,\n",
    "            trace_offset=trace_offset,\n",
    "            trace_scale=trace_scale)\n",
    "        syn.weight.data.fill_(1.)\n",
    "        # Forward\n",
    "        g = syn(hxsnn.NeuronHandle(inputs))\n",
    "        z = lif(g)\n",
    "        hxsnn.run(exp, 50)  # dt\n",
    "        traces.append(z.v_cadc.detach().numpy().reshape(-1))\n",
    "    return traces\n",
    "\n",
    "\n",
    "# Measure baseline\n",
    "inputs = torch.zeros((50, 1, 1))\n",
    "traces = run(inputs)\n",
    "baselines = np.stack(traces).mean()\n",
    "print(\"CADC membrane baseline: \", baselines)\n",
    "\n",
    "\n",
    "# Measure hardware threshold\n",
    "# We send a lot input spikes to excite the membrane potential towards the threshold potential\n",
    "# This will allow us to measure the real threshold value on hardware\n",
    "inputs = torch.ones((50, 1, 1))\n",
    "traces = run(inputs, trace_offset=baselines)\n",
    "bss2_threshold_real = np.stack(traces).max(1).mean()\n",
    "trace_scale = model_threshold / bss2_threshold_real\n",
    "print(\"Leak - Threshold on BSS-2: \", bss2_threshold_real)\n",
    "print(\"Trace scaling: \", trace_scale)\n",
    "\n",
    "\n",
    "# Next we tune the weight_scaling such that the PSP in the software model and on hardware look the same.\n",
    "# For this, we send exactly one input to compare the PSPs\n",
    "weight_scaling = 55.\n",
    "\n",
    "inputs = torch.zeros((50, 1, 1))\n",
    "inputs[10] = 1\n",
    "\n",
    "mock_trace = run(inputs, mock=True, n_runs=1)[0]\n",
    "bss2_traces = run(inputs, trace_offset=baselines, trace_scale=trace_scale, weight_scale=weight_scaling)\n",
    "\n",
    "# Display\n",
    "plot_scaled_trace(inputs, bss2_traces, mock_trace)\n",
    "\n",
    "hxtorch.release_hardware()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b103c",
   "metadata": {},
   "source": [
    "Now that we have aligned the dynamics on hardware and in the numerics, we can use the hardware to emulate out network and use the numerics to compute gradients for a given task.\n",
    "Training networks on BSS-2 using `hxtorch.snn` works the same as for plain PyTorch.\n",
    "Its easy… sometimes.\n",
    "\n",
    "In the remainder of this demo we will train a non-spiking leak-integrator (LI) output neuron to resemble a target trace.\n",
    "LI neuron layers are created by using `ReadoutNeurons`.\n",
    "\n",
    "As the target pattern we use a sine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17fb13",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def get_target(n_out):\n",
    "    targets = torch.zeros(100, 1, 3)\n",
    "    for n in range(n_out):\n",
    "        for i in range(3):\n",
    "            targets[:, 0, n] += torch.sign(torch.rand(1)-0.5)*torch.sin(\n",
    "                torch.linspace(0, (2 * torch.pi) / ((0.3 - 2) * torch.rand(1).item() + 2), 100))\n",
    "    norm = np.abs(targets).max(0)\n",
    "    return targets / norm.values\n",
    "\n",
    "targets = get_target(3)\n",
    "\n",
    "# Display\n",
    "plot_targets(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b1789",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as w\n",
    "\n",
    "hxtorch.init_hardware()\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "exp = hxsnn.Experiment(mock=False, calib_path='spiking_cocolist.pbin')\n",
    "\n",
    "lin1 = hxsnn.Synapse(\n",
    "    in_features=128,\n",
    "    out_features=3,\n",
    "    experiment=exp,\n",
    "    transform=partial(linear_saturating, scale=55))\n",
    "li = hxsnn.ReadoutNeuron(\n",
    "    3,\n",
    "    exp,\n",
    "    params=hxsnn.functional.CUBALIFParams(1./10e-6, 1./10e-6, v_leak=0, v_reset=0, v_th=1),\n",
    "    func=hxsnn.functional.cuba_li_integration,\n",
    "    shift_cadc_to_first=True,\n",
    "    trace_scale=trace_scale,\n",
    "    trace_offset=baselines)\n",
    "\n",
    "inputs = (torch.rand((100, 1, 128)) < 0.03).float()\n",
    "\n",
    "optimizer = torch.optim.Adam(lin1.parameters(), lr=2e-3)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "update_plot = plot_training(inputs, targets, EPOCHS)\n",
    "plt.close()\n",
    "output = w.Output()\n",
    "display(output)\n",
    "\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    g = lin1(hxsnn.NeuronHandle(inputs))\n",
    "    y = li(g)\n",
    "\n",
    "    # Run on BSS-2\n",
    "    hxsnn.run(exp, 100)\n",
    "\n",
    "    # Optimize\n",
    "    loss = loss_fn(y.v_cadc, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Plot\n",
    "    output.clear_output(wait=True)\n",
    "    with output:\n",
    "        update_plot(loss.item(), y)\n",
    "\n",
    "hxtorch.release_hardware()"
   ]
  }
 ],
 "metadata": {
  "date": 1741779497.4568295,
  "filename": "ts_11-hxtorch_snn_intro.rst",
  "kernelspec": {
   "display_name": "EBRAINS-experimental",
   "language": "python3",
   "name": "ebrains-experimental"
  },
  "title": "hxtorch.snn Introduction"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
